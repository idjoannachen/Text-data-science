{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "lab07.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSpr1qChKS0C",
        "colab_type": "text"
      },
      "source": [
        "# Lab 07: Movie Plots (2/2)\n",
        "In this lab we will continue working with topic models, but this time with a new dataset. Instead of abstracts of scientific articles, we will create topic models over movie plot descriptions. This is a dataset containing descriptions of movies from Wikipedia. The dataset was [obtained](https://www.kaggle.com/jrobischon/wikipedia-movie-plots) from Kaggle, an online community of data scientists.\n",
        "\n",
        "We will review the Counter class and how to build vocabularies again, and get practice with building topic models for new datasets. We'll also introduce the concepts of stemming and lemmatization. \n",
        "\n",
        "Spoiler alert! We will use the movie \"[Husbands and Wives](https://en.wikipedia.org/wiki/Husbands_and_Wives)\" as a running example...\n",
        "\n",
        "[<img width=200 src=\"https://upload.wikimedia.org/wikipedia/en/7/70/Husbands_moviep.jpg\">](https://en.wikipedia.org/wiki/Husbands_and_Wives)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khLikjg7KS0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datascience import *\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "logging.root.level = logging.CRITICAL \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n",
        "# direct plots to appear within the cell, and set their style\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plots\n",
        "plots.style.use('fivethirtyeight')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRnMtsdbKS0M",
        "colab_type": "text"
      },
      "source": [
        "This time around, the movie plot descriptions are in a CSV format in `movie_plots.csv`. The file is hosted on the Amazon Web Service s3. We'll use the `datascience` package to read this CSV file.\n",
        "\n",
        "The CSV file is rather large, so we'll randomly sample only 10000 rows, skipping over the others during the loading process. We could have sampled the rows after loading the entire CSV file, but this method saves memory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGjvj6a3KS0N",
        "colab_type": "code",
        "outputId": "069e6bb2-d68a-4219-b538-462ebcad5c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        }
      },
      "source": [
        "np.random.seed(101)\n",
        "\n",
        "total_size = 34886\n",
        "sample_size = 10000\n",
        "sample_rows = np.random.choice(range(1,total_size+2), sample_size, replace=False)\n",
        "skip_rows = np.setdiff1d(range(1,total_size+2), sample_rows)\n",
        "\n",
        "filename = \"https://s3.amazonaws.com/sds171/labs/lab07/movie_plots.csv\"\n",
        "data = Table.read_table(filename, skiprows=skip_rows)\n",
        "data.show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>Release Year</th> <th>Title</th> <th>Origin/Ethnicity</th> <th>Director</th> <th>Cast</th> <th>Genre</th> <th>Wiki Page</th> <th>Plot</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>1901        </td> <td>Kansas Saloon Smashers       </td> <td>American        </td> <td>Unknown           </td> <td>nan      </td> <td>unknown</td> <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Smashers        </td> <td>A bartender is working at a saloon, serving drinks to cu ...</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1901        </td> <td>Love by the Light of the Moon</td> <td>American        </td> <td>Unknown           </td> <td>nan      </td> <td>unknown</td> <td>https://en.wikipedia.org/wiki/Love_by_the_Light_of_the_Moon </td> <td>The moon, painted with a smiling face hangs over a park  ...</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1901        </td> <td>The Martyred Presidents      </td> <td>American        </td> <td>Unknown           </td> <td>nan      </td> <td>unknown</td> <td>https://en.wikipedia.org/wiki/The_Martyred_Presidents       </td> <td>The film, just over a minute long, is composed of two sh ...</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1903        </td> <td>Alice in Wonderland          </td> <td>American        </td> <td>Cecil Hepworth    </td> <td>May Clark</td> <td>unknown</td> <td>https://en.wikipedia.org/wiki/Alice_in_Wonderland_(1903_ ...</td> <td>Alice follows a large white rabbit down a \"Rabbit-hole\". ...</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1904        </td> <td>The Suburbanite              </td> <td>American        </td> <td>Wallace McCutcheon</td> <td>nan      </td> <td>comedy </td> <td>https://en.wikipedia.org/wiki/The_Suburbanite               </td> <td>The film is about a family who move to the suburbs, hopi ...</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>\n",
              "<p>... (9995 rows omitted)</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIbQPlBuKS0S",
        "colab_type": "text"
      },
      "source": [
        "The data is now in the `Table` format, which we are familiar with. We are only concerned with the plots, so we will extract them from the `Table`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BariOYFDKS0T",
        "colab_type": "code",
        "outputId": "a0ab0458-26eb-423c-f2fa-984984de8183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "sample = 3402\n",
        "\n",
        "plots = data.column('Plot')\n",
        "    \n",
        "print(\"Number of documents: %d\\n\" % len(plots))\n",
        "print(plots[sample])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 10000\n",
            "\n",
            "The film is about two couples: Jack (Pollack) and Sally (Davis), and Gabe (Allen) and Judy (Farrow). The film starts when Jack and Sally arrive at Gabe and Judy's apartment and announce their separation. Gabe is shocked, but Judy takes the news personally and is very hurt. Still confused, they go out for dinner at a Chinese restaurant.\r\n",
            "A few weeks later Sally goes to the apartment of a colleague. They plan to go out together to the opera and then to dinner. Sally asks if she can use his phone, and calls Jack. Learning from him that he has met someone, she accuses him of having had an affair during their marriage.\r\n",
            "Judy and Gabe are introduced to Jack's new girlfriend, Sam, an aerobics trainer. While Judy and Sam shop, Gabe calls Jack's new girlfriend a \"cocktail waitress\" and tells him that he is crazy for leaving Sally for her. About a week later, Judy introduces Sally to Michael (Neeson), Judy's magazine colleague who she clearly is interested in herself. Michael asks Sally out, and they begin dating; Michael is smitten, but Sally is dissatisfied with the relationship.\r\n",
            "Meanwhile, Gabe has developed a friendship with a young student of his, Rain, and has her read the manuscript of his novel. She comments on its brilliance, but has several criticisms, to which Gabe reacts defensively.\r\n",
            "At a party, Jack learns from a friend that Sally is seeing someone, and flies into a jealous rage. He and Sam break up after an intense argument, and Jack drives back to his house to find Sally in bed with Michael. He asks Sally to give their marriage another chance, but she tells him to leave.\r\n",
            "Less than two weeks later, however, Jack and Sally are back together and the couple meet Judy and Gabe for dinner like old times. After dinner, Judy and Gabe get into an argument about her not sharing her poetry. After Gabe makes a failed pass at her, Judy tells him she thinks the relationship is over; a week later Gabe moves out. Judy begins seeing Michael.\r\n",
            "Gabe goes to Rain's 21st birthday party, and gives her a music box as a present. She asks him to kiss her, and though the two share a romantic moment, Gabe tells her they should not pursue it any further. As he walks home in the rain, he realizes that he has ruined his relationship with Judy.\r\n",
            "Michael tells Judy he needs time alone, then says he can't help still having feelings for Sally. Angry and hurt, Judy walks out into the rain. Highlighting her \"passive aggressiveness,\" Michael follows and begs her to stay with him. A year and a half later they marry.\r\n",
            "At the end, the audience sees a pensive Jack and Sally back together. Jack and Sally admit their marital problems still exist (her frigidity is not solved), but they find they accept their problems as simply the price they have to pay to remain together.\r\n",
            "Gabe is living alone because he says he is not dating for the time being, as he does not want to hurt anyone. The film ends with an immediate cut to black after Gabe asks the unseen documentary crew, \"Can I go? Is this over?\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0l6aP-KKS0Y",
        "colab_type": "text"
      },
      "source": [
        "This plot description is from the movie \"[Husbands and Wives](https://en.wikipedia.org/wiki/Husbands_and_Wives)\"\n",
        "\n",
        "We don't have LaTeX markup in these documents, but we'll still use some regular expressions to do some simpe pre-processing of punctuation. There are lots of names in the plot descriptions, so we'll remove all the words that have a capitalized first letter. This will remove lots of non-name words as well, but this'll be sufficient for our goal of building a basic topic model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPwojbkkKS0a",
        "colab_type": "code",
        "outputId": "7d7d7630-002d-4c2b-8c0b-66124f38d3b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# replace '-' with ' ', then remove punctuation\n",
        "plots = [re.sub('-', ' ', plot) for plot in plots]\n",
        "plots = [re.sub('[^\\w\\s]', '', plot) for plot in plots]\n",
        "\n",
        "# remove tokens with a capitalized first letter \n",
        "# (broad stroke to remove names)\n",
        "plots = [re.sub('[A-Z]\\w*', '', plot) for plot in plots]\n",
        "# replace multiple spaces by a single space\n",
        "plots = [re.sub('[ ]+', ' ', plot) for plot in plots]\n",
        "\n",
        "print(plots[sample])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " film is about two couples and and and film starts when and arrive at and apartment and announce their separation is shocked but takes the news personally and is very hurt confused they go out for dinner at a restaurant\r\n",
            " few weeks later goes to the apartment of a colleague plan to go out together to the opera and then to dinner asks if she can use his phone and calls from him that he has met someone she accuses him of having had an affair during their marriage\r\n",
            " and are introduced to new girlfriend an aerobics trainer and shop calls new girlfriend a cocktail waitress and tells him that he is crazy for leaving for her a week later introduces to magazine colleague who she clearly is interested in herself asks out and they begin dating is smitten but is dissatisfied with the relationship\r\n",
            " has developed a friendship with a young student of his and has her read the manuscript of his novel comments on its brilliance but has several criticisms to which reacts defensively\r\n",
            " a party learns from a friend that is seeing someone and flies into a jealous rage and break up after an intense argument and drives back to his house to find in bed with asks to give their marriage another chance but she tells him to leave\r\n",
            " than two weeks later however and are back together and the couple meet and for dinner like old times dinner and get into an argument about her not sharing her poetry makes a failed pass at her tells him she thinks the relationship is over a week later moves out begins seeing \r\n",
            " goes to 21st birthday party and gives her a music box as a present asks him to kiss her and though the two share a romantic moment tells her they should not pursue it any further he walks home in the rain he realizes that he has ruined his relationship with \r\n",
            " tells he needs time alone then says he cant help still having feelings for and hurt walks out into the rain her passive aggressiveness follows and begs her to stay with him year and a half later they marry\r\n",
            " the end the audience sees a pensive and back together and admit their marital problems still exist her frigidity is not solved but they find they accept their problems as simply the price they have to pay to remain together\r\n",
            " is living alone because he says he is not dating for the time being as he does not want to hurt anyone film ends with an immediate cut to black after asks the unseen documentary crew go this over\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1ET-RKyKS0e",
        "colab_type": "text"
      },
      "source": [
        "Now, we further process each plot description by converting it to lower case, stripping leading and trailing white space, and then tokenizing by splitting on spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWDlcJmmKS0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plots_tok = []\n",
        "for plot in plots:\n",
        "    processed = plot.lower().strip().split(' ')\n",
        "    plots_tok.append(processed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RdTLwXfKS0k",
        "colab_type": "text"
      },
      "source": [
        "As in Lab06, we will remove tokens that have digits, possessives or contractions, or are empty strings.\n",
        "- `is_numeric(string)` checks if `string` has any numbers\n",
        "- `has_poss_contr(string)` checks if `string` has possessives or contractions\n",
        "- `empty_string(string)` checks if `string` is an empty string\n",
        "- `remove_string(string)` checcks if `string` should be removed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5FMzR5IKS0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_numeric(string):\n",
        "    for char in string:\n",
        "        if char.isdigit():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def has_poss_contr(string):\n",
        "    for i in range(len(string) - 1):\n",
        "        if string[i] == '\\'' and string[i+1] == 's':\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def empty_string(string):\n",
        "    return string == ''\n",
        "\n",
        "def remove_string(string):\n",
        "    return is_numeric(string) | has_poss_contr(string) | empty_string(string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhzrKd5iKS0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = []\n",
        "for plot in plots_tok:\n",
        "    filtered = []\n",
        "    for token in plot:\n",
        "        if not remove_string(token):\n",
        "            filtered.append(token)\n",
        "    temp.append(filtered)\n",
        "plots_tok = temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOtQUvGQKS0v",
        "colab_type": "text"
      },
      "source": [
        "Recall that to build topic models, we require the following components:\n",
        "- A vocabulary of tokens that appear across all documents.\n",
        "- A mapping of those tokens to a unique integer identifier, because topic model algorithms treat words by these identifiers, and not the strings themselves. For example, we represent `'epidemic'` as `word2id['epidemic'] = 50`\n",
        "- The corpus, where each document in the corpus is a collection of tokens, where each token is represented by the identifier and the number of times it appears in the document. For example, in the first document above the token `'epidemic'`, which appears twice, is represented as `(50, 2)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2etMo1ZAKS0w",
        "colab_type": "text"
      },
      "source": [
        "First, we will build a vocabulary representing the tokens that have appeared across all the plot descriptions we have. When doing this, we typically want to (1) remove rare words, (2) remove stop words and (3) stem/lemmatize words.\n",
        "\n",
        "Stemming and lemmatization are techniques to derive the root form of a word (`'run'`), given an inflected form of the word (`'running'`).\n",
        "\n",
        "**Stemming**. This is the process of shortening a word based on common heuristics, such as removing suffixes. For example, `'appeared'` -> `'appear'`. Sometimes, there is nothing to remove, like in `'saw'` -> `'saw'`.\n",
        "\n",
        "**Lemmatization**. This is the process of deriving the root word based on its part of speech and word morphology. For example, `'saw'` -> `'saw'` if it is used as a noun, but `'saw'` -> `'see'` if it is used as a verb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_vRPTK7KS0y",
        "colab_type": "code",
        "outputId": "270c8742-3289-454d-fd4b-bfbca96b44ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "temp = []\n",
        "for plot in plots_tok:\n",
        "    processed = []\n",
        "    for token in plot:\n",
        "        processed.append(lemmatizer.lemmatize(token, pos='v'))\n",
        "    temp.append(processed)\n",
        "plots_tok = temp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5jRIxsoKS02",
        "colab_type": "text"
      },
      "source": [
        "Recall that we can use the `Counter` class to build the vocabulary. The `Counter` is an extension of the Python dictionary, and also has key-value pairs. For the `Counter`, keys are the objects to be counted, while values are their counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyaG5WarKS03",
        "colab_type": "code",
        "outputId": "a1e4a7b3-097c-4ff8-8d0e-aef43f20dd61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = Counter()\n",
        "for plot in plots_tok:\n",
        "    vocab.update(plot)\n",
        "\n",
        "print(\"Number of unique tokens: %d\" % len(vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique tokens: 33686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3DBPvPDKS07",
        "colab_type": "text"
      },
      "source": [
        "Recall that removing rare words helps prevent our vocabulary from being too large. Many tokens appear only a few times across all the plot descriptions. Keeping them in the vocabulary increases subsequent computation time. Furthermore, their presence tends not to carry much significance for a document, since they can be considered as anomalies.\n",
        "\n",
        "We remove rare words by only keeping tokens that appear more than 25 times across all plot descriptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uZ266DBKS08",
        "colab_type": "code",
        "outputId": "11392f9d-1e22-47b7-f9db-df7ffe8a5426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokens = []\n",
        "for token in vocab.elements():\n",
        "    if vocab[token] > 25:\n",
        "        tokens.append(token)\n",
        "vocab = Counter(tokens)\n",
        "\n",
        "print(\"Number of unique tokens: %d\" % len(vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique tokens: 5586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGzrYR9SKS1A",
        "colab_type": "text"
      },
      "source": [
        "Recall that stop words are defined as very common words such as `'the'` and `'a'`. Removing stop words is important because their presence also does not carry much significance, since they appear in all kinds of texts.\n",
        "\n",
        "We will remove stop words by removing the 200 most common tokens across all the plot descriptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31-D8ORVKS1B",
        "colab_type": "code",
        "outputId": "8b147d2c-2ed1-48b9-d13e-1f7fb2ca11be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stop_words = []\n",
        "for item in vocab.most_common(200):\n",
        "    stop_word = item[0]\n",
        "    stop_words.append(stop_word)\n",
        "tokens = []\n",
        "for token in vocab.elements():\n",
        "    if token not in stop_words:\n",
        "        tokens.append(token)\n",
        "vocab = Counter(tokens)\n",
        "\n",
        "print(\"Number of unique tokens: %d\" % len(vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique tokens: 5386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVk_6i1zKS1F",
        "colab_type": "text"
      },
      "source": [
        "Now we create a mapping for tokens to unique identifiers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV93XZgdKS1G",
        "colab_type": "code",
        "outputId": "eab6597a-b04f-4bcd-f10b-10fcf0154962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "items = vocab.items()\n",
        "id2word = {}\n",
        "word2id = {}\n",
        "idx = 0\n",
        "for word, count in vocab.items():\n",
        "    id2word[idx] = word\n",
        "    word2id[word] = idx\n",
        "    idx += 1\n",
        "    \n",
        "print(\"Number of tokens mapped: %d\" % len(id2word))\n",
        "print(\"Identifier for 'photograph': %d\" % word2id['photograph'])\n",
        "print(\"Word for identifier %d: %s\" % (word2id['photograph'], id2word[word2id['photograph']]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tokens mapped: 5386\n",
            "Identifier for 'photograph': 2152\n",
            "Word for identifier 2152: photograph\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJx-JbCLKS1L",
        "colab_type": "text"
      },
      "source": [
        "Now, we will remove, for each plot description, the tokens that are not found in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkSc60y5KS1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = []\n",
        "for plot in plots_tok:\n",
        "    filtered = []\n",
        "    for token in plot:\n",
        "        if token in vocab:\n",
        "            filtered.append(token)\n",
        "    temp.append(filtered)\n",
        "plots_tok = temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTQJ0mwJKS1Q",
        "colab_type": "text"
      },
      "source": [
        "Let's create the corpus. Recall that for the tokens in a plot description:\n",
        "\n",
        "```\n",
        "['doctor', 'pearson', 'michael', 'rennie', 'canadian', 'hospital', 'province', 'quebec', 'receive', 'series', 'poison', 'pen', 'letter', 'letter', 'sign', 'mysterious', 'picture', 'feather', 'deliver', 'others', 'small', 'canadian', 'cora', 'laurent', 'constance', 'smith', 'main', 'doctor', 'dr', 'laurent', 'charles', 'boyer', 'hospital', 'receive', 'letter', 'accuse', 'affair', 'pearson', 'letter', 'inform', 'shell', 'shock', 'veteran', 'mr', 'cancer', 'distraught', 'commit', 'suicide', 'quickly', 'townsfolk', 'point', 'finger', 'possible', 'suspect'] \n",
        "```\n",
        "the corpus has the format\n",
        "```\n",
        "[(1841, 2), (2095, 2), (2096, 1), (2097, 1), (2098, 2), (105, 2), (2099, 1), (2100, 1), (270, 2), (1763, 1), (1870, 1), (2101, 1), (2017, 4), (633, 1), (1270, 1), (1093, 1), (2102, 1), (1197, 1), (113, 1), (1583, 1), (2103, 1), (2104, 2), (2105, 1), (873, 1), (1950, 1), (107, 1), (2106, 1), (2107, 1), (116, 1), (1436, 1), (62, 1), (2108, 1), (213, 1), (2109, 1), (1205, 1), (2110, 1), (1042, 1), (1275, 1), (1259, 1), (1342, 1), (2111, 1), (440, 1), (1662, 1), (374, 1), (663, 1)]\n",
        "```\n",
        "\n",
        "where each element is a pair containing the identifier for the token and the count of that token in just that plot description."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf0-NzbFKS1R",
        "colab_type": "code",
        "outputId": "1fbda735-e512-46cd-eac3-b50d02919f3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "corpus = []\n",
        "for plot in plots_tok:\n",
        "    plot_count = Counter(plot)\n",
        "    corpus_doc = []\n",
        "    for item in plot_count.items():\n",
        "        pair = (word2id[item[0]], item[1])\n",
        "        corpus_doc.append(pair)\n",
        "    corpus.append(corpus_doc)\n",
        "\n",
        "print(\"Plot, tokenized:\\n\", plots_tok[sample], \"\\n\")\n",
        "print(\"Plot, in corpus format:\\n\", corpus[sample])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Plot, tokenized:\n",
            " ['couple', 'apartment', 'announce', 'separation', 'shock', 'news', 'personally', 'very', 'hurt', 'confuse', 'dinner', 'few', 'weeks', 'apartment', 'colleague', 'together', 'opera', 'dinner', 'phone', 'someone', 'accuse', 'affair', 'marriage\\r\\n', 'introduce', 'girlfriend', 'trainer', 'shop', 'girlfriend', 'waitress', 'crazy', 'week', 'introduce', 'magazine', 'colleague', 'clearly', 'interest', 'herself', 'date', 'smite', 'relationship\\r\\n', 'develop', 'friendship', 'student', 'read', 'manuscript', 'novel', 'comment', 'several', 'react', 'party', 'someone', 'fly', 'jealous', 'rage', 'intense', 'argument', 'bed', 'marriage', 'chance', 'leave\\r\\n', 'than', 'weeks', 'however', 'together', 'couple', 'dinner', 'dinner', 'argument', 'share', 'poetry', 'fail', 'pass', 'think', 'relationship', 'week', 'birthday', 'party', 'music', 'box', 'present', 'kiss', 'though', 'share', 'romantic', 'moment', 'should', 'pursue', 'any', 'further', 'walk', 'rain', 'ruin', 'relationship', 'need', 'alone', 'cant', 'feel', 'hurt', 'walk', 'rain', 'beg', 'stay', 'year', 'half', 'audience', 'together', 'admit', 'marital', 'problems', 'exist', 'solve', 'accept', 'problems', 'simply', 'price', 'pay', 'remain', 'together\\r\\n', 'alone', 'date', 'hurt', 'anyone', 'immediate', 'cut', 'black', 'unseen', 'documentary', 'crew'] \n",
            "\n",
            "Plot, in corpus format:\n",
            " [(33, 2), (1915, 2), (1142, 1), (3261, 1), (569, 1), (502, 1), (2553, 1), (353, 1), (3301, 3), (3116, 1), (1749, 4), (1411, 1), (1459, 2), (3433, 2), (260, 3), (1912, 1), (2490, 1), (1476, 2), (625, 1), (1337, 1), (1347, 1), (129, 2), (1072, 2), (2247, 1), (1638, 1), (2734, 1), (4110, 1), (1960, 2), (1559, 1), (4160, 1), (454, 1), (611, 1), (3088, 2), (2471, 1), (3498, 1), (540, 1), (603, 1), (1621, 1), (257, 1), (4973, 1), (397, 1), (4288, 1), (581, 1), (3389, 1), (192, 2), (932, 1), (698, 1), (309, 1), (3765, 1), (2551, 2), (139, 1), (459, 1), (2113, 1), (3606, 1), (1235, 1), (532, 1), (1488, 2), (4158, 1), (463, 1), (81, 1), (369, 1), (615, 2), (1453, 1), (1988, 1), (119, 1), (146, 1), (299, 1), (1441, 1), (204, 1), (205, 1), (1396, 1), (478, 1), (794, 1), (677, 1), (34, 2), (2101, 2), (425, 1), (1013, 1), (1229, 2), (169, 1), (1048, 1), (213, 1), (1420, 1), (674, 1), (433, 1), (991, 1), (1798, 1), (3631, 1), (3113, 2), (3663, 1), (2629, 1), (281, 1), (489, 1), (1114, 1), (269, 1), (1136, 1), (3299, 1), (1093, 1), (3219, 1), (867, 1), (164, 1), (560, 1), (4678, 1), (188, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONMT7iDFKS1U",
        "colab_type": "text"
      },
      "source": [
        "Now, we are ready to create our topic model!\n",
        "\n",
        "We again use gensim, a Python library to create topic models. Also, we again use the algorithm called latent dirichlet allocation implemented in the gensim library. \n",
        "\n",
        "**This step takes about 40s**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF6x_JbDKS1V",
        "colab_type": "code",
        "outputId": "390b96e5-fc05-44fc-8b70-1fcf71c2a0c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                            id2word=id2word,\n",
        "                                            num_topics=10, \n",
        "                                            random_state=100,\n",
        "                                            update_every=1,\n",
        "                                            chunksize=100,\n",
        "                                            passes=10,\n",
        "                                            alpha='auto',\n",
        "                                            per_word_topics=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 36.9 s, sys: 11.4 ms, total: 36.9 s\n",
            "Wall time: 36.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VATyjFrmKS1a",
        "colab_type": "text"
      },
      "source": [
        "After building the topic model, we want to view the 10 topics. The topics are represented as a combination of keywords with corresponding weight on the keyword. Note that the order of these topics can change between different training runs of the topic model, since there is no ordering between topics and gensim returns them in an arbitrary order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MExWCPtzKS1c",
        "colab_type": "code",
        "outputId": "d6037756-d229-463c-c2ad-86b3fa568db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "num_topics = 10\n",
        "num_words = 15\n",
        "top_words = Table().with_column('word rank', np.arange(1,num_words+1))\n",
        "for k in np.arange(num_topics): \n",
        "    topic = lda_model.get_topic_terms(k, num_words)\n",
        "    words = [id2word[topic[i][0]] for i in np.arange(num_words)]\n",
        "    probs = [topic[i][1] for i in np.arange(num_words)]\n",
        "    top_words = top_words.with_column('topic %d' % k, words)\n",
        "    \n",
        "top_words.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>word rank</th> <th>topic 0</th> <th>topic 1</th> <th>topic 2</th> <th>topic 3</th> <th>topic 4</th> <th>topic 5</th> <th>topic 6</th> <th>topic 7</th> <th>topic 8</th> <th>topic 9</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>1        </td> <td>officer</td> <td>win     </td> <td>power     </td> <td>village  </td> <td>marriage</td> <td>movie    </td> <td>gang    </td> <td>ship      </td> <td>battle    </td> <td>body    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2        </td> <td>case   </td> <td>team    </td> <td>destroy   </td> <td>sister   </td> <td>feel    </td> <td>accident </td> <td>company </td> <td>island    </td> <td>city      </td> <td>appear  </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>3        </td> <td>arrest </td> <td>college </td> <td>world     </td> <td>hospital </td> <td>how     </td> <td>girls    </td> <td>local   </td> <td>matter    </td> <td>rescue    </td> <td>enter   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>4        </td> <td>dead   </td> <td>high    </td> <td>child     </td> <td>student  </td> <td>parent  </td> <td>dream    </td> <td>business</td> <td>project   </td> <td>land      </td> <td>open    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>5        </td> <td>though </td> <td>join    </td> <td>boy       </td> <td>doctor   </td> <td>very    </td> <td>rich     </td> <td>revenge </td> <td>sea       </td> <td>travel    </td> <td>search  </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>6        </td> <td>commit </td> <td>students</td> <td>children  </td> <td>villagers</td> <td>stay    </td> <td>act      </td> <td>brothers</td> <td>boat      </td> <td>defeat    </td> <td>chase   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>7        </td> <td>leader </td> <td>members </td> <td>baby      </td> <td>spirit   </td> <td>together</td> <td>around   </td> <td>kidnap  </td> <td>million   </td> <td>encounter </td> <td>hide    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>8        </td> <td>truth  </td> <td>final   </td> <td>ghost     </td> <td>teacher  </td> <td>happen  </td> <td>character</td> <td>sell    </td> <td>wealth    </td> <td>human     </td> <td>throw   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>9        </td> <td>former </td> <td>second  </td> <td>always    </td> <td>uncle    </td> <td>accept  </td> <td>perform  </td> <td>jail    </td> <td>crew      </td> <td>soldier   </td> <td>reach   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>10       </td> <td>under  </td> <td>henchmen</td> <td>create    </td> <td>due      </td> <td>too     </td> <td>scene    </td> <td>law     </td> <td>launch    </td> <td>temple    </td> <td>inside  </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>11       </td> <td>suspect</td> <td>events  </td> <td>transform </td> <td>study    </td> <td>finally </td> <td>part     </td> <td>sisters </td> <td>passengers</td> <td>mysterious</td> <td>suddenly</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>12       </td> <td>witness</td> <td>game    </td> <td>everything</td> <td>form     </td> <td>think   </td> <td>real     </td> <td>phone   </td> <td>beach     </td> <td>capture   </td> <td>wait    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>13       </td> <td>war    </td> <td>base    </td> <td>bear      </td> <td>elder    </td> <td>change  </td> <td>whose    </td> <td>boss    </td> <td>treasure  </td> <td>army      </td> <td>trap    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>14       </td> <td>claim  </td> <td>fail    </td> <td>forest    </td> <td>teach    </td> <td>never   </td> <td>music    </td> <td>crime   </td> <td>ocean     </td> <td>control   </td> <td>water   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>15       </td> <td>secret </td> <td>series  </td> <td>person    </td> <td>past     </td> <td>good    </td> <td>different</td> <td>threaten</td> <td>advantage </td> <td>form      </td> <td>catch   </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "224387Z9KS1g",
        "colab_type": "text"
      },
      "source": [
        "We can compute the probability distribution for a given plot description in the `corpus`. This represents how likely it is for the plot description to belong to each topic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgDLBWqZKS1h",
        "colab_type": "code",
        "outputId": "8641c95c-dd43-4c77-9ced-d6817e6dfc96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "sample = 3402\n",
        "lda_model.get_document_topics(corpus[sample], minimum_probability=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0.0799971),\n",
              " (1, 0.0836261),\n",
              " (2, 0.04086707),\n",
              " (3, 0.034000255),\n",
              " (4, 0.5298888),\n",
              " (5, 0.08152593),\n",
              " (6, 0.06373515),\n",
              " (7, 0.010502912),\n",
              " (8, 0.024644673),\n",
              " (9, 0.051212043)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlSciprxKS1j",
        "colab_type": "text"
      },
      "source": [
        "Let's represent this as a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5YzQRvDKS1k",
        "colab_type": "code",
        "outputId": "cf8f111f-3cf8-456c-d9f4-f3b04cfcbf2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "topic_dist = lda_model.get_document_topics(corpus[sample], minimum_probability=0)\n",
        "topics = [pair[0] for pair in topic_dist]\n",
        "probabilities = [pair[1] for pair in topic_dist]\n",
        "topic_dist_table = Table().with_columns('Topic', topics, 'Probabilities', probabilities)\n",
        "topic_dist_table.show(20)\n",
        "t = np.argmax(probabilities)\n",
        "print(\"Topic with highest probability: %d (%f)\" % (t, probabilities[t]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>Topic</th> <th>Probabilities</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>0    </td> <td>0.0799951    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1    </td> <td>0.0836254    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2    </td> <td>0.0408732    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>3    </td> <td>0.0340003    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>4    </td> <td>0.529889     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>5    </td> <td>0.0815253    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>6    </td> <td>0.0637311    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>7    </td> <td>0.0105029    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>8    </td> <td>0.0246447    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>9    </td> <td>0.0512127    </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Topic with highest probability: 4 (0.529889)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au0Sma71KS1o",
        "colab_type": "text"
      },
      "source": [
        "And now as a histogram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYtOnuIsKS1p",
        "colab_type": "code",
        "outputId": "0ba11b49-df69-44cd-976a-df0148b920d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(11,4)\n",
        "plt.bar(topic_dist_table.column('Topic'), topic_dist_table.column('Probabilities'), align='center', alpha=1, color='salmon')\n",
        "plt.xlabel('topic')\n",
        "plt.ylabel('probability')\n",
        "plt.title('Per Topic Probability Distribution')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAEXCAYAAAAHlko2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 792x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6nVpJrpKS1t",
        "colab_type": "text"
      },
      "source": [
        "So it seems the movie plot description\n",
        "\n",
        "```\n",
        "The film is about two couples: Jack (Pollack) and Sally (Davis), and Gabe (Allen) and Judy (Farrow). The film starts when Jack and Sally arrive at Gabe and Judy's apartment and announce their separation. Gabe is shocked, but Judy takes the news personally and is very hurt. Still confused, they go out for dinner at a Chinese restaurant.\n",
        "A few weeks later Sally goes to the apartment of a colleague. They plan to go out together to the opera and then to dinner. Sally asks if she can use his phone, and calls Jack. Learning from him that he has met someone, she accuses him of having had an affair during their marriage.\n",
        "Judy and Gabe are introduced to Jack's new girlfriend, Sam, an aerobics trainer. While Judy and Sam shop, Gabe calls Jack's new girlfriend a \"cocktail waitress\" and tells him that he is crazy for leaving Sally for her. About a week later, Judy introduces Sally to Michael (Neeson), Judy's magazine colleague who she clearly is interested in herself. Michael asks Sally out, and they begin dating; Michael is smitten, but Sally is dissatisfied with the relationship.\n",
        "Meanwhile, Gabe has developed a friendship with a young student of his, Rain, and has her read the manuscript of his novel. She comments on its brilliance, but has several criticisms, to which Gabe reacts defensively.\n",
        "At a party, Jack learns from a friend that Sally is seeing someone, and flies into a jealous rage. He and Sam break up after an intense argument, and Jack drives back to his house to find Sally in bed with Michael. He asks Sally to give their marriage another chance, but she tells him to leave.\n",
        "Less than two weeks later, however, Jack and Sally are back together and the couple meet Judy and Gabe for dinner like old times. After dinner, Judy and Gabe get into an argument about her not sharing her poetry. After Gabe makes a failed pass at her, Judy tells him she thinks the relationship is over; a week later Gabe moves out. Judy begins seeing Michael.\n",
        "Gabe goes to Rain's 21st birthday party, and gives her a music box as a present. She asks him to kiss her, and though the two share a romantic moment, Gabe tells her they should not pursue it any further. As he walks home in the rain, he realizes that he has ruined his relationship with Judy.\n",
        "Michael tells Judy he needs time alone, then says he can't help still having feelings for Sally. Angry and hurt, Judy walks out into the rain. Highlighting her \"passive aggressiveness,\" Michael follows and begs her to stay with him. A year and a half later they marry.\n",
        "At the end, the audience sees a pensive Jack and Sally back together. Jack and Sally admit their marital problems still exist (her frigidity is not solved), but they find they accept their problems as simply the price they have to pay to remain together.\n",
        "Gabe is living alone because he says he is not dating for the time being, as he does not want to hurt anyone. The film ends with an immediate cut to black after Gabe asks the unseen documentary crew, \"Can I go? Is this over?\"\n",
        "```\n",
        "\n",
        "has the greatest likelihood to fall under the topic number with topic relating to relationships, which matches our intuition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiJ2UgaQKS1t",
        "colab_type": "text"
      },
      "source": [
        "## Topic Models for TED Talk Transcripts\n",
        "\n",
        "*Your turn!* Your task is broken down into two parts.\n",
        "\n",
        "#### 1. Run Topic Models\n",
        "You are given a new dataset of transcripts of TED Talks. This dataset is also [obtained](https://www.kaggle.com/rounakbanik/ted-talks) from Kaggle. You can use the following link to load the data, in a CSV file `ted_talks.csv`, from s3:\n",
        "\n",
        "```\n",
        "filename = \"https://s3.amazonaws.com/sds171/labs/lab07/ted_talks.csv\"\n",
        "data = Table.read_table(filename)\n",
        "```\n",
        "\n",
        "Run the code above to train a topic model over this new dataset. In particular,\n",
        "- Load the CSV file. You can simply use the two lines of code above, without skipping any rows.\n",
        "- Preprocess\n",
        "    - Discard some transcripts if you find it appropriate.\n",
        "    - Add a regular expression here if you find it is appropriate.\n",
        "    - Tokenize and remove numerics, possessives/contractions, and empty strings\n",
        "    - Lemmatize tokens\n",
        "- Create a vocabulary using a `Counter`\n",
        "- Filter the vocabulary by removing rare words and stop words. You should be getting a final vocabulary size of about 7000.\n",
        "- Create the identifier mappings `word2id` and `id2word`\n",
        "- Filter tokens from the tokenized data using the final vocabulary\n",
        "- Create the corpus\n",
        "- Train the topic model\n",
        "- Show the topics along with the top words\n",
        "\n",
        "You should find yourself simply using the code given above for this new dataset. Very few changes are necessary!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwss-2spKS1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"https://s3.amazonaws.com/sds171/labs/lab07/ted_talks.csv\"\n",
        "data = Table.read_table(filename)\n",
        "\n",
        "plots = data.column('transcript')\n",
        "\n",
        "plots = [re.sub('-', ' ', plot) for plot in plots]\n",
        "plots = [re.sub('[^\\w\\s]', '', plot) for plot in plots]\n",
        "\n",
        "# remove tokens with a capitalized first letter \n",
        "# (broad stroke to remove names)\n",
        "plots = [re.sub('[A-Z]\\w*', '', plot) for plot in plots]\n",
        "# replace multiple spaces by a single space\n",
        "plots = [re.sub('[ ]+', ' ', plot) for plot in plots]\n",
        "\n",
        "plots_tok = []\n",
        "for plot in plots:\n",
        "    processed = plot.lower().strip().split(' ')\n",
        "    plots_tok.append(processed)\n",
        "\n",
        "temp = []\n",
        "for plot in plots_tok:\n",
        "    filtered = []\n",
        "    for token in plot:\n",
        "        if not remove_string(token):\n",
        "            filtered.append(token)\n",
        "    temp.append(filtered)\n",
        "plots_tok = temp\n",
        "\n",
        "temp = []\n",
        "for plot in plots_tok:\n",
        "    processed = []\n",
        "    for token in plot:\n",
        "        processed.append(lemmatizer.lemmatize(token, pos='v'))\n",
        "    temp.append(processed)\n",
        "plots_tok = temp\n",
        "\n",
        "vocab2 = Counter()\n",
        "for plot in plots_tok:\n",
        "    vocab2.update(plot)\n",
        "\n",
        "tokens = []\n",
        "for token in vocab2.elements():\n",
        "    if vocab2[token] > 20:\n",
        "        tokens.append(token)\n",
        "vocab2 = Counter(tokens)\n",
        "\n",
        "stop_words = []\n",
        "for item in vocab2.most_common(101):\n",
        "    stop_word = item[0]\n",
        "    stop_words.append(stop_word)\n",
        "tokens = []\n",
        "for token in vocab2.elements():\n",
        "    if token not in stop_words:\n",
        "        tokens.append(token)\n",
        "vocab2 = Counter(tokens)\n",
        "\n",
        "items = vocab2.items()\n",
        "id2word = {}\n",
        "word2id = {}\n",
        "idx = 0\n",
        "for word, count in vocab2.items():\n",
        "    id2word[idx] = word\n",
        "    word2id[word] = idx\n",
        "    idx += 1\n",
        "\n",
        "temp = []\n",
        "for plot in plots_tok:\n",
        "    filtered = []\n",
        "    for token in plot:\n",
        "        if token in vocab2:\n",
        "            filtered.append(token)\n",
        "    temp.append(filtered)\n",
        "plots_tok = temp\n",
        "\n",
        "corpus = []\n",
        "for plot in plots_tok:\n",
        "    plot_count = Counter(plot)\n",
        "    corpus_doc = []\n",
        "    for item in plot_count.items():\n",
        "        pair = (word2id[item[0]], item[1])\n",
        "        corpus_doc.append(pair)\n",
        "    corpus.append(corpus_doc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgw761RVO7qa",
        "colab_type": "code",
        "outputId": "4e205ee4-8f07-4f94-f53a-a3808d989d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        }
      },
      "source": [
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                            id2word=id2word,\n",
        "                                            num_topics=10, \n",
        "                                            random_state=100,\n",
        "                                            update_every=1,\n",
        "                                            chunksize=100,\n",
        "                                            passes=10,\n",
        "                                            alpha='auto',\n",
        "                                            per_word_topics=True)\n",
        "\n",
        "num_topics = 10\n",
        "num_words = 30\n",
        "top_words = Table().with_column('word rank', np.arange(1,num_words+1))\n",
        "for k in np.arange(num_topics): \n",
        "    topic = lda_model.get_topic_terms(k, num_words)\n",
        "    words = [id2word[topic[i][0]] for i in np.arange(num_words)]\n",
        "    probs = [topic[i][1] for i in np.arange(num_words)]\n",
        "    top_words = top_words.with_column('topic %d' % k, words)\n",
        "\n",
        "top_words.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>word rank</th> <th>topic 0</th> <th>topic 1</th> <th>topic 2</th> <th>topic 3</th> <th>topic 4</th> <th>topic 5</th> <th>topic 6</th> <th>topic 7</th> <th>topic 8</th> <th>topic 9</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>1        </td> <td>brain    </td> <td>data       </td> <td>space    </td> <td>water  </td> <td>health   </td> <td>youre  </td> <td>her    </td> <td>mean      </td> <td>percent   </td> <td>build     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2        </td> <td>body     </td> <td>technology </td> <td>light    </td> <td>live   </td> <td>kid      </td> <td>lot    </td> <td>she    </td> <td>question  </td> <td>company   </td> <td>play      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>3        </td> <td>cells    </td> <td>information</td> <td>universe </td> <td>plant  </td> <td>school   </td> <td>thing  </td> <td>his    </td> <td>human     </td> <td>money     </td> <td>feel      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>4        </td> <td>cancer   </td> <td>machine    </td> <td>image    </td> <td>species</td> <td>children </td> <td>little </td> <td>women  </td> <td>most      </td> <td>countries </td> <td>place     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>5        </td> <td>different</td> <td>computer   </td> <td>planet   </td> <td>food   </td> <td>care     </td> <td>theres </td> <td>him    </td> <td>change    </td> <td>country   </td> <td>his       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>6        </td> <td>cell     </td> <td>new        </td> <td>star     </td> <td>climate</td> <td>learn    </td> <td>down   </td> <td>life   </td> <td>even      </td> <td>change    </td> <td>love      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>7        </td> <td>human    </td> <td>build      </td> <td>physics  </td> <td>fish   </td> <td>her      </td> <td>back   </td> <td>love   </td> <td>feel      </td> <td>new       </td> <td>little    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>8        </td> <td>blood    </td> <td>design     </td> <td>life     </td> <td>change </td> <td>live     </td> <td>theyre </td> <td>day    </td> <td>many      </td> <td>government</td> <td>day       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>9        </td> <td>new      </td> <td>learn      </td> <td>science  </td> <td>planet </td> <td>doctor   </td> <td>happen </td> <td>men    </td> <td>experience</td> <td>build     </td> <td>through   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>10       </td> <td>able     </td> <td>game       </td> <td>energy   </td> <td>carbon </td> <td>medical  </td> <td>good   </td> <td>live   </td> <td>live      </td> <td>global    </td> <td>back      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>11       </td> <td>lab      </td> <td>create     </td> <td>solar    </td> <td>forest </td> <td>percent  </td> <td>try    </td> <td>never  </td> <td>same      </td> <td>create    </td> <td>create    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>12       </td> <td>also     </td> <td>show       </td> <td>around   </td> <td>place  </td> <td>every    </td> <td>put    </td> <td>didnt  </td> <td>understand</td> <td>also      </td> <td>sound     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>13       </td> <td>genes    </td> <td>phone      </td> <td>planets  </td> <td>tree   </td> <td>patients </td> <td>bite   </td> <td>story  </td> <td>different </td> <td>dollars   </td> <td>live      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>14       </td> <td>genetic  </td> <td>also       </td> <td>cloud    </td> <td>animals</td> <td>help     </td> <td>talk   </td> <td>man    </td> <td>give      </td> <td>today     </td> <td>show      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>15       </td> <td>gene     </td> <td>different  </td> <td>sun      </td> <td>also   </td> <td>baby     </td> <td>off    </td> <td>talk   </td> <td>ask       </td> <td>most      </td> <td>walk      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>16       </td> <td>same     </td> <td>video      </td> <td>black    </td> <td>ocean  </td> <td>drug     </td> <td>kind   </td> <td>family </td> <td>should    </td> <td>many      </td> <td>around    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>17       </td> <td>process  </td> <td>over       </td> <td>number   </td> <td>grow   </td> <td>most     </td> <td>well   </td> <td>back   </td> <td>good      </td> <td>city      </td> <td>design    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>18       </td> <td>system   </td> <td>tool       </td> <td>object   </td> <td>over   </td> <td>year     </td> <td>guy    </td> <td>only   </td> <td>might     </td> <td>year      </td> <td>kind      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>19       </td> <td>example  </td> <td>digital    </td> <td>only     </td> <td>most   </td> <td>life     </td> <td>car    </td> <td>young  </td> <td>also      </td> <td>public    </td> <td>experience</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>20       </td> <td>weve     </td> <td>example    </td> <td>sky      </td> <td>life   </td> <td>cancer   </td> <td>stuff  </td> <td>even   </td> <td>talk      </td> <td>cities    </td> <td>over      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>21       </td> <td>disease  </td> <td>real       </td> <td>hole     </td> <td>land   </td> <td>better   </td> <td>cant   </td> <td>ask    </td> <td>word      </td> <td>job       </td> <td>art       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>22       </td> <td>through  </td> <td>lot        </td> <td>telescope</td> <td>sea    </td> <td>disease  </td> <td>didnt  </td> <td>woman  </td> <td>life      </td> <td>business  </td> <td>space     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>23       </td> <td>show     </td> <td>network    </td> <td>different</td> <td>only   </td> <td>study    </td> <td>youve  </td> <td>after  </td> <td>social    </td> <td>pay       </td> <td>also      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>24       </td> <td>part     </td> <td>online     </td> <td>system   </td> <td>back   </td> <td>high     </td> <td>run    </td> <td>feel   </td> <td>why       </td> <td>only      </td> <td>life      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>25       </td> <td>tissue   </td> <td>even       </td> <td>away     </td> <td>year   </td> <td>die      </td> <td>around </td> <td>meet   </td> <td>believe   </td> <td>market    </td> <td>new       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>26       </td> <td>structure</td> <td>understand </td> <td>field    </td> <td>energy </td> <td>parent   </td> <td>over   </td> <td>write  </td> <td>own       </td> <td>million   </td> <td>become    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>27       </td> <td>immune   </td> <td>able       </td> <td>may      </td> <td>air    </td> <td>child    </td> <td>problem</td> <td>help   </td> <td>may       </td> <td>even      </td> <td>music     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>28       </td> <td>change   </td> <td>help       </td> <td>particles</td> <td>eat    </td> <td>she      </td> <td>pretty </td> <td>name   </td> <td>become    </td> <td>live      </td> <td>thing     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>29       </td> <td>control  </td> <td>every      </td> <td>even     </td> <td>down   </td> <td>education</td> <td>turn   </td> <td>home   </td> <td>between   </td> <td>power     </td> <td>every     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>30       </td> <td>move     </td> <td>try        </td> <td>size     </td> <td>natural</td> <td>give     </td> <td>mean   </td> <td>every  </td> <td>mind      </td> <td>over      </td> <td>move      </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-UN_J7bSzUG",
        "colab_type": "code",
        "outputId": "7a695b1e-aac4-4df4-84af-5fbd3e35fae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "sample = 986\n",
        "lda_model.get_document_topics(corpus[sample], minimum_probability=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0.05426892),\n",
              " (1, 0.0045721037),\n",
              " (2, 0.0013454031),\n",
              " (3, 0.0025268),\n",
              " (4, 0.0033183408),\n",
              " (5, 0.09980144),\n",
              " (6, 0.21806023),\n",
              " (7, 0.014545581),\n",
              " (8, 0.006270614),\n",
              " (9, 0.5952906)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nooqT80O_bC",
        "colab_type": "code",
        "outputId": "817cb76e-7159-4377-bba0-821d1bd00ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "topic_dist = lda_model.get_document_topics(corpus[sample], minimum_probability=0)\n",
        "topics = [pair[0] for pair in topic_dist]\n",
        "probabilities = [pair[1] for pair in topic_dist]\n",
        "topic_dist_table = Table().with_columns('Topic', topics, 'Probabilities', probabilities)\n",
        "topic_dist_table.show(20)\n",
        "t = np.argmax(probabilities)\n",
        "print(\"Topic with highest probability: %d (%f)\" % (t, probabilities[t]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>Topic</th> <th>Probabilities</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>0    </td> <td>0.0542789    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1    </td> <td>0.00457211   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2    </td> <td>0.0013454    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>3    </td> <td>0.0025268    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>4    </td> <td>0.00331826   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>5    </td> <td>0.0997937    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>6    </td> <td>0.218061     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>7    </td> <td>0.0145424    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>8    </td> <td>0.00627061   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>9    </td> <td>0.595291     </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Topic with highest probability: 9 (0.595291)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nIwyXaZPVfh",
        "colab_type": "code",
        "outputId": "6b9802f6-8f82-4cf5-c0bc-595e7dfcde6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.set_size_inches(11,4)\n",
        "plt.bar(topic_dist_table.column('Topic'), topic_dist_table.column('Probabilities'), align='center', alpha=1, color='salmon')\n",
        "plt.xlabel('topic')\n",
        "plt.ylabel('probability')\n",
        "plt.title('Per Topic Probability Distribution')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAEXCAYAAAAHlko2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkqSOGMYlSZKkjhjGJUmSpI78f400Zkbm2m0GAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 792x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqsMHI5iUUM4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "#### 2. Discussion\n",
        "Discuss the results. Choose two or three articles and describe how the most probable topic does or does not seem to accurately represent the main theme of the paper. Include your comments in Markdown cell, with code cells added as needed to pull out particular rows of your table. You may find it useful to copy over the code for `create_topic_table` from the previous lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNgkNEZbP601",
        "colab_type": "code",
        "outputId": "05fd5c9e-5e8e-49b1-e4b9-a8486765244b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(plots[1314])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " grew up in on the border of and a little town called and electricity supply are as unpredictable as the weather and growing up in these tough situations at the age of 17 was relaxing with a couple of friends of mine in winter and we were sunbathing sun gets really hot in winter as we were sunbathing my best friend next to me says why doesnt somebody invent something that you can just put on your skin and then you dont have to bathe sat and was like would buy that eh went home and did a little research and found some very shocking statistics 25 billion people in the world today do not have proper access to water and sanitation hundred and fifty million of them are in and five million of them are in diseases thrive in this environment the most drastic of which is called trachoma is an infection of the eye due to dirt getting into your eye infections of trachoma can leave you permanently blind disease leaves eight million people permanently blind each and every year shocking part about it is that to avoid being infected with trachoma all you have to do is wash your face no medicine no pills no injections after seeing these shocking statistics thought to myself even if not just doing it for myself and the fact that dont want to bathe at least need to do it to try to save the world with my trusty little steed my 6234 cell phone didnt have a laptop didnt have much except for the 20 rand an hour cafe did research on on about lotions creams the compositions the melting points the toxicities did high school science and wrote down a little formula on a piece of paper and it looked like the special spice you know was like okay so weve got the formula ready we need to get this thing into practice forward four years later after having written a 40 page business plan on the cell phone having written my patent on the cell phone the youngest patent holder in the country and more bathing cant say any more than that had invented the worlds first bath substituting lotion literally put it on your skin and you dont have to bathe after having tried to make it work in high school with the limited resources had went to university met a few people got it into practice and we have a fully functioning product thats ready to go to the market actually available on the market we learned a few lessons in commercializing and making available of the things we learned was that poor communities dont buy products in bulk buy products on demand person in doesnt buy a box of cigarettes buy one cigarette each day even though its more expensive we packaged in these innovative little sachets just snap them in half and you squeeze it out the cool part is one sachet substitutes one bath for five rand creating that model we also learned a lot in terms of implementing the product realized that even rich kids from the suburbs really want least once a week we realized that we could save 80 million liters of water on average each time they skipped a bath and also we would save two hours a day for kids who are in rural areas two hours more for school two hours more for homework two hours more to just be a kid seeing that global impact we narrowed it down to our key value proposition which was cleanliness and convenience is a rich mans convenience and a poor mans lifesaver put the product into practice we are actually now on the verge of selling the product onto a multinational to take it to the retail market and one question have for the audience today is on the gravel roads of with an allowance of 50 rand a week came up with a way for the world not to bathe stopping you not done yet not done yet another key thing that learned a lot throughout this whole process last year named me as one of the brightest young minds in the world also currently the best student entrepreneur in the world the first to get that accolade and one thing that really puzzles me is did all of this just because didnt want to bathe you \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9gqclU0jrUS",
        "colab_type": "code",
        "outputId": "e49e6b23-6b66-4864-b087-81a741dcd250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(plots[986])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " dont understand myself why they keep talking of love if they come near me if they look into my eyes and kiss my hand dont understand myself why they talk of magic that no one withstands if he sees me if he passes by if the red light is on in the middle of the night and everybody listens to my song then it is plain to see lips they give so fiery a kiss my limbs they are supple and soft is written for me in the stars thou shalt kiss thou shalt love feet they glide and float my eyes they lure and glow dance as if entranced cause know my lips give so fiery a kiss my veins runs a dancers blood because my beautiful mother was the of dance in the gilded was so very beautiful often saw her in my dreams she beat the tambourine to her beguiling dance all eyes were glowing admiringly reawakened in me mine is the same lot dance like her at midnight and from deep within feel lips they give so fiery a kiss my limbs they are supple and soft is written for me in the stars thou shalt kiss thou shalt love dance as if entranced cause know my lips give so fiery a kiss \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D9jcgr7ktsX",
        "colab_type": "text"
      },
      "source": [
        "For the first talk, I chose sample number 1314. It was predicted to be topic 8 which has top words related to economics and government. The talk has the speaker tell a story about how sunbathing and leading to some people in the world being unable to bathe regularly due to a lack of access to water. The talk continues with the speaker doing research on lotions and creating a product and business plan that will enable people to put on a lotion that will be equivalent to bathing. This talk reflects topic 8, since topic 8 relates to economics, and the talk relates to building a business."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKzzB-dyonA5",
        "colab_type": "text"
      },
      "source": [
        "The second talk is sample number 986. It was predicted to be topic 9 which is related to building, playing, and loving. The talk, however, speaks on kissing. Love is mentioned, but love is also a top word in topic 6, which has the second highest probability. It is hard to determine what would be the unquestionable best topic for this based solely on the transcript and the 15 top words, so the 30 top words were examined. Topic 9 seems to be all about different experiences and with the talk relating to the vivid experience of kissing, the topic seems to fit."
      ]
    }
  ]
}